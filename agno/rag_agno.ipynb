{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from together import Together\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import together\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"TOGETHER_API_KEY\")\n",
    "\n",
    "# Initialize the InferenceClient with together as the provider\n",
    "\n",
    "client = together.Together(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing and Chunking\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_pg_essay():\n",
    "\n",
    "    url = 'https://paulgraham.com/foundermode.html'\n",
    "\n",
    "    try:\n",
    "        # Send GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Paul Graham's essays typically have the main content in a font tag\n",
    "        # You might need to adjust this selector based on the actual HTML structure\n",
    "        content = soup.find('font')\n",
    "\n",
    "        if content:\n",
    "            # Extract and clean the text\n",
    "            text = content.get_text()\n",
    "            # Remove extra whitespace and normalize line breaks\n",
    "            text = ' '.join(text.split())\n",
    "            return text\n",
    "        else:\n",
    "            return \"Could not find the main content of the essay.\"\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error fetching the webpage: {e}\"\n",
    "\n",
    "# Scrape the essay\n",
    "pg_essay = scrape_pg_essay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c14bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive fixed sized chunking with overlaps\n",
    "\n",
    "def create_chunks(document, chunk_size=300, overlap=50):\n",
    "    return [document[i : i + chunk_size] for i in range(0, len(document), chunk_size - overlap)]\n",
    "  \n",
    "  \n",
    "chunks = create_chunks(pg_essay, chunk_size=250, overlap=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c303ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Vector Index and Perform Retrieval\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def generate_embeddings(input_texts: List[str], model_api_string: str) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings from Together python library.\n",
    "\n",
    "    Args:\n",
    "        input_texts: a list of string input texts.\n",
    "        model_api_string: str. An API string for a specific embedding model of your choice.\n",
    "\n",
    "    Returns:\n",
    "        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n",
    "    \"\"\"\n",
    "    outputs = client.embeddings.create(\n",
    "        input=input_texts,\n",
    "        model=model_api_string,\n",
    "    )\n",
    "    return np.array([x.embedding for x in outputs.data])\n",
    "  \n",
    "embeddings = generate_embeddings(chunks, \"BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b3bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_retreival(query: str, top_k: int = 5, vector_index: np.ndarray = None) -> List[int]:\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most similar items from an index based on a query.\n",
    "    Args:\n",
    "        query (str): The query string to search for.\n",
    "        top_k (int, optional): The number of top similar items to retrieve. Defaults to 5.\n",
    "        index (np.ndarray, optional): The index array containing embeddings to search against. Defaults to None.\n",
    "    Returns:\n",
    "        List[int]: A list of indices corresponding to the top-k most similar items in the index.\n",
    "    \"\"\"\n",
    "\n",
    "    query_embedding = np.array(generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0])\n",
    "\n",
    "    similarity_scores = np.dot(query_embedding, vector_index.T)\n",
    "\n",
    "    return list(np.argsort(-similarity_scores)[:top_k])\n",
    "  \n",
    "top_k_indices = vector_retreival(query =  \"What are 'skip-level' meetings?\", top_k = 5, vector_index = embeddings)\n",
    "top_k_chunks = [chunks[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank To Improve Quality\n",
    "\n",
    "\n",
    "def rerank(query: str, chunks: List[str], top_k = 3) -> List[int]:\n",
    "\n",
    "    response = client.rerank.create(\n",
    "    model = \"Salesforce/Llama-Rank-V1\",\n",
    "    query = query,\n",
    "    documents = chunks,\n",
    "    top_n = top_k\n",
    "    )\n",
    "\n",
    "    return [result.index for result in response.results]\n",
    "\n",
    "rerank_indices = rerank(\"What are 'skip-level' meetings?\", chunks = top_k_chunks, top_k=3)\n",
    "\n",
    "reranked_chunks = ''\n",
    "\n",
    "for index in rerank_indices:\n",
    "    reranked_chunks += top_k_chunks[index] + '\\n\\n'\n",
    "\n",
    "print(reranked_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Generative Model - Llama 405b\n",
    "\n",
    "query = \"What are 'skip-level' meetings?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n",
    "      {\"role\": \"user\", \"content\": f\"Answer the question: {query}. Use only information provided here: {reranked_chunks}\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c15452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
